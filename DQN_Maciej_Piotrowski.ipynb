{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968402a9",
   "metadata": {},
   "source": [
    "# Final Agent - Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32b174c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_100 (Flatten)       (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_200 (Dense)           (None, 100)               500       \n",
      "                                                                 \n",
      " dense_201 (Dense)           (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 702\n",
      "Trainable params: 702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/300, score: 39, e: 1.0\n",
      "episode: 1/300, score: 13, e: 1.0\n",
      "episode: 2/300, score: 17, e: 1.0\n",
      "episode: 3/300, score: 14, e: 1.0\n",
      "episode: 4/300, score: 31, e: 1.0\n",
      "episode: 5/300, score: 15, e: 1.0\n",
      "episode: 6/300, score: 26, e: 1.0\n",
      "episode: 7/300, score: 11, e: 1.0\n",
      "episode: 8/300, score: 15, e: 1.0\n",
      "episode: 9/300, score: 32, e: 1.0\n",
      "episode: 10/300, score: 29, e: 1.0\n",
      "episode: 11/300, score: 33, e: 1.0\n",
      "episode: 12/300, score: 12, e: 1.0\n",
      "episode: 13/300, score: 11, e: 1.0\n",
      "episode: 14/300, score: 12, e: 1.0\n",
      "episode: 15/300, score: 15, e: 1.0\n",
      "episode: 16/300, score: 26, e: 1.0\n",
      "episode: 17/300, score: 52, e: 1.0\n",
      "episode: 18/300, score: 11, e: 1.0\n",
      "episode: 19/300, score: 10, e: 1.0\n",
      "episode: 20/300, score: 16, e: 1.0\n",
      "episode: 21/300, score: 47, e: 1.0\n",
      "episode: 22/300, score: 37, e: 1.0\n",
      "episode: 23/300, score: 24, e: 1.0\n",
      "episode: 24/300, score: 21, e: 1.0\n",
      "episode: 25/300, score: 35, e: 1.0\n",
      "episode: 26/300, score: 8, e: 1.0\n",
      "episode: 27/300, score: 17, e: 1.0\n",
      "episode: 28/300, score: 14, e: 1.0\n",
      "episode: 29/300, score: 20, e: 1.0\n",
      "episode: 30/300, score: 20, e: 1.0\n",
      "episode: 31/300, score: 27, e: 1.0\n",
      "episode: 32/300, score: 12, e: 1.0\n",
      "episode: 33/300, score: 51, e: 1.0\n",
      "episode: 34/300, score: 32, e: 1.0\n",
      "episode: 35/300, score: 22, e: 1.0\n",
      "episode: 36/300, score: 35, e: 1.0\n",
      "episode: 37/300, score: 13, e: 1.0\n",
      "episode: 38/300, score: 22, e: 1.0\n",
      "episode: 39/300, score: 29, e: 1.0\n",
      "episode: 40/300, score: 15, e: 1.0\n",
      "episode: 41/300, score: 16, e: 1.0\n",
      "episode: 42/300, score: 20, e: 1.0\n",
      "episode: 43/300, score: 29, e: 0.94\n",
      "episode: 44/300, score: 26, e: 0.72\n",
      "episode: 45/300, score: 10, e: 0.66\n",
      "episode: 46/300, score: 9, e: 0.6\n",
      "episode: 47/300, score: 10, e: 0.54\n",
      "episode: 48/300, score: 8, e: 0.5\n",
      "episode: 49/300, score: 13, e: 0.44\n",
      "episode: 50/300, score: 15, e: 0.38\n",
      "episode: 51/300, score: 10, e: 0.34\n",
      "episode: 52/300, score: 16, e: 0.29\n",
      "episode: 53/300, score: 19, e: 0.24\n",
      "episode: 54/300, score: 13, e: 0.21\n",
      "episode: 55/300, score: 12, e: 0.19\n",
      "episode: 56/300, score: 11, e: 0.17\n",
      "episode: 57/300, score: 8, e: 0.15\n",
      "episode: 58/300, score: 10, e: 0.14\n",
      "episode: 59/300, score: 11, e: 0.12\n",
      "episode: 60/300, score: 10, e: 0.11\n",
      "episode: 61/300, score: 13, e: 0.099\n",
      "episode: 62/300, score: 49, e: 0.061\n",
      "episode: 63/300, score: 10, e: 0.055\n",
      "episode: 64/300, score: 39, e: 0.037\n",
      "episode: 65/300, score: 10, e: 0.033\n",
      "episode: 66/300, score: 12, e: 0.03\n",
      "episode: 67/300, score: 8, e: 0.027\n",
      "episode: 68/300, score: 10, e: 0.025\n",
      "episode: 69/300, score: 10, e: 0.022\n",
      "episode: 70/300, score: 9, e: 0.02\n",
      "episode: 71/300, score: 10, e: 0.019\n",
      "episode: 72/300, score: 11, e: 0.017\n",
      "episode: 73/300, score: 9, e: 0.015\n",
      "episode: 74/300, score: 9, e: 0.014\n",
      "episode: 75/300, score: 10, e: 0.013\n",
      "episode: 76/300, score: 10, e: 0.011\n",
      "episode: 77/300, score: 9, e: 0.01\n",
      "episode: 78/300, score: 9, e: 0.0099\n",
      "episode: 79/300, score: 8, e: 0.0099\n",
      "episode: 80/300, score: 13, e: 0.0099\n",
      "episode: 81/300, score: 39, e: 0.0099\n",
      "episode: 82/300, score: 49, e: 0.0099\n",
      "episode: 83/300, score: 38, e: 0.0099\n",
      "episode: 84/300, score: 40, e: 0.0099\n",
      "episode: 85/300, score: 64, e: 0.0099\n",
      "episode: 86/300, score: 269, e: 0.0099\n",
      "episode: 87/300, score: 64, e: 0.0099\n",
      "episode: 88/300, score: 59, e: 0.0099\n",
      "episode: 89/300, score: 71, e: 0.0099\n",
      "episode: 90/300, score: 67, e: 0.0099\n",
      "episode: 91/300, score: 70, e: 0.0099\n",
      "episode: 92/300, score: 54, e: 0.0099\n",
      "episode: 93/300, score: 71, e: 0.0099\n",
      "episode: 94/300, score: 71, e: 0.0099\n",
      "episode: 95/300, score: 77, e: 0.0099\n",
      "episode: 96/300, score: 117, e: 0.0099\n",
      "episode: 97/300, score: 95, e: 0.0099\n",
      "episode: 98/300, score: 252, e: 0.0099\n",
      "episode: 99/300, score: 100, e: 0.0099\n",
      "episode: 100/300, score: 67, e: 0.0099\n",
      "episode: 101/300, score: 64, e: 0.0099\n",
      "episode: 102/300, score: 188, e: 0.0099\n",
      "episode: 103/300, score: 56, e: 0.0099\n",
      "episode: 104/300, score: 71, e: 0.0099\n",
      "episode: 105/300, score: 133, e: 0.0099\n",
      "episode: 106/300, score: 60, e: 0.0099\n",
      "episode: 107/300, score: 75, e: 0.0099\n",
      "episode: 108/300, score: 404, e: 0.0099\n",
      "episode: 109/300, score: 201, e: 0.0099\n",
      "episode: 110/300, score: 262, e: 0.0099\n",
      "episode: 111/300, score: 394, e: 0.0099\n",
      "episode: 112/300, score: 121, e: 0.0099\n",
      "episode: 113/300, score: 73, e: 0.0099\n",
      "episode: 114/300, score: 278, e: 0.0099\n",
      "episode: 115/300, score: 91, e: 0.0099\n",
      "episode: 116/300, score: 98, e: 0.0099\n",
      "episode: 117/300, score: 148, e: 0.0099\n",
      "episode: 118/300, score: 500, e: 0.0099\n",
      "episode: 0/20, score: 86\n",
      "episode: 1/20, score: 147\n",
      "episode: 2/20, score: 173\n",
      "episode: 3/20, score: 500\n",
      "episode: 4/20, score: 206\n",
      "episode: 5/20, score: 170\n",
      "episode: 6/20, score: 500\n",
      "episode: 7/20, score: 282\n",
      "episode: 8/20, score: 500\n",
      "episode: 9/20, score: 500\n",
      "episode: 10/20, score: 500\n",
      "episode: 11/20, score: 500\n",
      "episode: 12/20, score: 500\n",
      "episode: 13/20, score: 500\n",
      "episode: 14/20, score: 500\n",
      "episode: 15/20, score: 500\n",
      "episode: 16/20, score: 97\n",
      "episode: 17/20, score: 163\n",
      "episode: 18/20, score: 85\n",
      "episode: 19/20, score: 500\n",
      "Average score over 20 runs : 345.45\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "#Creating the environment of the game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#Defining the neural network model\n",
    "def NNModel(states, actions):\n",
    "    \n",
    "    #Input layer\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(states)))\n",
    "    \n",
    "    #Hidden layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    \n",
    "    #Output layer\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    \n",
    "    #Configures the model for training\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Our agent\n",
    "class DQNAgent:\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        self.states = env.observation_space.shape[0]\n",
    "        self.actions = env.action_space.n\n",
    "        \n",
    "        #Parameters\n",
    "        self.max_ep = 300\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma=0.95\n",
    "        self.eps = 1.0\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.model=NNModel(states=(self.states,), actions = self.actions)\n",
    "    \n",
    "    #Adding an entry to our buffer\n",
    "    def saveToBuff(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > 1000:\n",
    "            if self.eps > 0.01:\n",
    "                #Decreasing eps (the probability to take a random action)\n",
    "                self.eps = self.eps * 0.99\n",
    "\n",
    "    #Trains the model with experiences from memory\n",
    "    def replay(self):\n",
    "        if len(self.memory) < 1000:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.states))\n",
    "        next_state = np.zeros((self.batch_size, self.states))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # Batch prediction\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Update the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "    \n",
    "    def run(self):\n",
    "        for e in range(self.max_ep):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.states])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #Uncomment to display the task \n",
    "                #self.env.render()\n",
    "                \n",
    "                #Pick a random action if random between 0 and 1 is smaller than eps\n",
    "                if np.random.random() <= self.eps:\n",
    "                    action = random.randrange(self.actions)\n",
    "                else:\n",
    "                    #Pick the best possible action\n",
    "                    action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.states])\n",
    "                #if the CartPole is still not out or it's the before last step\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.saveToBuff(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                #If the CartPole is out or the score reached 500\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.max_ep, i, self.eps))\n",
    "                    #If the max score is reached\n",
    "                    if i == 500:\n",
    "                        #Saving the model\n",
    "                        self.model.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "    \n",
    "    def test(self):\n",
    "            #Loading the model\n",
    "            self.model = load_model(\"cartpole-dqn.h5\")\n",
    "            SA=0\n",
    "            for e in range(20):\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.states])\n",
    "                done = False\n",
    "                i = 0\n",
    "                while not done:\n",
    "                    #Uncomment to display the task\n",
    "                    #self.env.render()\n",
    "                    action = np.argmax(self.model.predict(state))\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    state = np.reshape(next_state, [1, self.states])\n",
    "                    i += 1\n",
    "                    if done:\n",
    "                        print(\"episode: {}/{}, score: {}\".format(e, 20, i))\n",
    "                        SA+=i\n",
    "                        break\n",
    "            print(\"Average score over 20 runs : \"+str(SA/20))\n",
    "            self.env.close()\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgent(env)\n",
    "    agent.run()\n",
    "    agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d2d61",
   "metadata": {},
   "source": [
    "# Final Agent -  Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f983910e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_101 (Flatten)       (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_202 (Dense)           (None, 100)               300       \n",
      "                                                                 \n",
      " dense_203 (Dense)           (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 603\n",
      "Trainable params: 603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/300, score: -200, e: 1.0\n",
      "episode: 1/300, score: -200, e: 1.0\n",
      "episode: 2/300, score: -200, e: 1.0\n",
      "episode: 3/300, score: -200, e: 1.0\n",
      "episode: 4/300, score: -200, e: 1.0\n",
      "episode: 5/300, score: -200, e: 0.82\n",
      "episode: 6/300, score: -200, e: 0.67\n",
      "episode: 7/300, score: -200, e: 0.55\n",
      "episode: 8/300, score: -200, e: 0.45\n",
      "episode: 9/300, score: -200, e: 0.37\n",
      "episode: 10/300, score: -200, e: 0.3\n",
      "episode: 11/300, score: -200, e: 0.25\n",
      "episode: 12/300, score: -200, e: 0.2\n",
      "episode: 13/300, score: -200, e: 0.17\n",
      "episode: 14/300, score: -200, e: 0.14\n",
      "episode: 15/300, score: -200, e: 0.11\n",
      "episode: 16/300, score: -200, e: 0.091\n",
      "episode: 17/300, score: -200, e: 0.074\n",
      "episode: 18/300, score: -200, e: 0.061\n",
      "episode: 19/300, score: -200, e: 0.05\n",
      "episode: 20/300, score: -200, e: 0.041\n",
      "episode: 21/300, score: -200, e: 0.033\n",
      "episode: 22/300, score: -200, e: 0.027\n",
      "episode: 23/300, score: -200, e: 0.022\n",
      "episode: 24/300, score: -200, e: 0.018\n",
      "episode: 25/300, score: -200, e: 0.015\n",
      "episode: 26/300, score: -200, e: 0.012\n",
      "episode: 27/300, score: -200, e: 0.01\n",
      "episode: 28/300, score: -200, e: 0.01\n",
      "episode: 29/300, score: -200, e: 0.01\n",
      "episode: 30/300, score: -200, e: 0.01\n",
      "episode: 31/300, score: -200, e: 0.01\n",
      "episode: 32/300, score: -200, e: 0.01\n",
      "episode: 33/300, score: -200, e: 0.01\n",
      "episode: 34/300, score: -200, e: 0.01\n",
      "episode: 35/300, score: -200, e: 0.01\n",
      "episode: 36/300, score: -157, e: 0.01\n",
      "episode: 37/300, score: -200, e: 0.01\n",
      "episode: 38/300, score: -200, e: 0.01\n",
      "episode: 39/300, score: -200, e: 0.01\n",
      "episode: 40/300, score: -200, e: 0.01\n",
      "episode: 41/300, score: -200, e: 0.01\n",
      "episode: 42/300, score: -200, e: 0.01\n",
      "episode: 43/300, score: -200, e: 0.01\n",
      "episode: 44/300, score: -200, e: 0.01\n",
      "episode: 45/300, score: -200, e: 0.01\n",
      "episode: 46/300, score: -200, e: 0.01\n",
      "episode: 47/300, score: -200, e: 0.01\n",
      "episode: 48/300, score: -200, e: 0.01\n",
      "episode: 49/300, score: -200, e: 0.01\n",
      "episode: 50/300, score: -200, e: 0.01\n",
      "episode: 51/300, score: -169, e: 0.01\n",
      "episode: 52/300, score: -200, e: 0.01\n",
      "episode: 53/300, score: -200, e: 0.01\n",
      "episode: 54/300, score: -200, e: 0.01\n",
      "episode: 55/300, score: -200, e: 0.01\n",
      "episode: 56/300, score: -200, e: 0.01\n",
      "episode: 57/300, score: -200, e: 0.01\n",
      "episode: 58/300, score: -200, e: 0.01\n",
      "episode: 59/300, score: -200, e: 0.01\n",
      "episode: 60/300, score: -200, e: 0.01\n",
      "episode: 61/300, score: -200, e: 0.01\n",
      "episode: 62/300, score: -200, e: 0.01\n",
      "episode: 63/300, score: -200, e: 0.01\n",
      "episode: 64/300, score: -200, e: 0.01\n",
      "episode: 65/300, score: -200, e: 0.01\n",
      "episode: 66/300, score: -200, e: 0.01\n",
      "episode: 67/300, score: -200, e: 0.01\n",
      "episode: 68/300, score: -200, e: 0.01\n",
      "episode: 69/300, score: -200, e: 0.01\n",
      "episode: 70/300, score: -200, e: 0.01\n",
      "episode: 71/300, score: -200, e: 0.01\n",
      "episode: 72/300, score: -200, e: 0.01\n",
      "episode: 73/300, score: -200, e: 0.01\n",
      "episode: 74/300, score: -200, e: 0.01\n",
      "episode: 75/300, score: -200, e: 0.01\n",
      "episode: 76/300, score: -200, e: 0.01\n",
      "episode: 77/300, score: -200, e: 0.01\n",
      "episode: 78/300, score: -200, e: 0.01\n",
      "episode: 79/300, score: -200, e: 0.01\n",
      "episode: 80/300, score: -200, e: 0.01\n",
      "episode: 81/300, score: -200, e: 0.01\n",
      "episode: 82/300, score: -200, e: 0.01\n",
      "episode: 83/300, score: -200, e: 0.01\n",
      "episode: 84/300, score: -200, e: 0.01\n",
      "episode: 85/300, score: -200, e: 0.01\n",
      "episode: 86/300, score: -200, e: 0.01\n",
      "episode: 87/300, score: -200, e: 0.01\n",
      "episode: 88/300, score: -200, e: 0.01\n",
      "episode: 89/300, score: -200, e: 0.01\n",
      "episode: 90/300, score: -200, e: 0.01\n",
      "episode: 91/300, score: -200, e: 0.01\n",
      "episode: 92/300, score: -200, e: 0.01\n",
      "episode: 93/300, score: -200, e: 0.01\n",
      "episode: 94/300, score: -200, e: 0.01\n",
      "episode: 95/300, score: -200, e: 0.01\n",
      "episode: 96/300, score: -200, e: 0.01\n",
      "episode: 97/300, score: -200, e: 0.01\n",
      "episode: 98/300, score: -200, e: 0.01\n",
      "episode: 99/300, score: -200, e: 0.01\n",
      "episode: 100/300, score: -200, e: 0.01\n",
      "episode: 101/300, score: -200, e: 0.01\n",
      "episode: 102/300, score: -200, e: 0.01\n",
      "episode: 103/300, score: -200, e: 0.01\n",
      "episode: 104/300, score: -200, e: 0.01\n",
      "episode: 105/300, score: -200, e: 0.01\n",
      "episode: 106/300, score: -200, e: 0.01\n",
      "episode: 107/300, score: -200, e: 0.01\n",
      "episode: 108/300, score: -200, e: 0.01\n",
      "episode: 109/300, score: -200, e: 0.01\n",
      "episode: 110/300, score: -200, e: 0.01\n",
      "episode: 111/300, score: -200, e: 0.01\n",
      "episode: 112/300, score: -200, e: 0.01\n",
      "episode: 113/300, score: -200, e: 0.01\n",
      "episode: 114/300, score: -200, e: 0.01\n",
      "episode: 115/300, score: -200, e: 0.01\n",
      "episode: 116/300, score: -200, e: 0.01\n",
      "episode: 117/300, score: -200, e: 0.01\n",
      "episode: 118/300, score: -200, e: 0.01\n",
      "episode: 119/300, score: -200, e: 0.01\n",
      "episode: 120/300, score: -200, e: 0.01\n",
      "episode: 121/300, score: -200, e: 0.01\n",
      "episode: 122/300, score: -200, e: 0.01\n",
      "episode: 123/300, score: -200, e: 0.01\n",
      "episode: 124/300, score: -200, e: 0.01\n",
      "episode: 125/300, score: -200, e: 0.01\n",
      "episode: 126/300, score: -200, e: 0.01\n",
      "episode: 127/300, score: -200, e: 0.01\n",
      "episode: 128/300, score: -200, e: 0.01\n",
      "episode: 129/300, score: -200, e: 0.01\n",
      "episode: 130/300, score: -200, e: 0.01\n",
      "episode: 131/300, score: -200, e: 0.01\n",
      "episode: 132/300, score: -200, e: 0.01\n",
      "episode: 133/300, score: -200, e: 0.01\n",
      "episode: 134/300, score: -200, e: 0.01\n",
      "episode: 135/300, score: -200, e: 0.01\n",
      "episode: 136/300, score: -200, e: 0.01\n",
      "episode: 137/300, score: -200, e: 0.01\n",
      "episode: 138/300, score: -200, e: 0.01\n",
      "episode: 139/300, score: -200, e: 0.01\n",
      "episode: 140/300, score: -200, e: 0.01\n",
      "episode: 141/300, score: -200, e: 0.01\n",
      "episode: 142/300, score: -200, e: 0.01\n",
      "episode: 143/300, score: -200, e: 0.01\n",
      "episode: 144/300, score: -200, e: 0.01\n",
      "episode: 145/300, score: -200, e: 0.01\n",
      "episode: 146/300, score: -200, e: 0.01\n",
      "episode: 147/300, score: -200, e: 0.01\n",
      "episode: 148/300, score: -200, e: 0.01\n",
      "episode: 149/300, score: -200, e: 0.01\n",
      "episode: 150/300, score: -200, e: 0.01\n",
      "episode: 151/300, score: -200, e: 0.01\n",
      "episode: 152/300, score: -200, e: 0.01\n",
      "episode: 153/300, score: -200, e: 0.01\n",
      "episode: 154/300, score: -200, e: 0.01\n",
      "episode: 155/300, score: -200, e: 0.01\n",
      "episode: 156/300, score: -200, e: 0.01\n",
      "episode: 157/300, score: -200, e: 0.01\n",
      "episode: 158/300, score: -200, e: 0.01\n",
      "episode: 159/300, score: -200, e: 0.01\n",
      "episode: 160/300, score: -200, e: 0.01\n",
      "episode: 161/300, score: -200, e: 0.01\n",
      "episode: 162/300, score: -200, e: 0.01\n",
      "episode: 163/300, score: -200, e: 0.01\n",
      "episode: 164/300, score: -200, e: 0.01\n",
      "episode: 165/300, score: -200, e: 0.01\n",
      "episode: 166/300, score: -200, e: 0.01\n",
      "episode: 167/300, score: -200, e: 0.01\n",
      "episode: 168/300, score: -200, e: 0.01\n",
      "episode: 169/300, score: -200, e: 0.01\n",
      "episode: 170/300, score: -200, e: 0.01\n",
      "episode: 171/300, score: -200, e: 0.01\n",
      "episode: 172/300, score: -200, e: 0.01\n",
      "episode: 173/300, score: -200, e: 0.01\n",
      "episode: 174/300, score: -200, e: 0.01\n",
      "episode: 175/300, score: -200, e: 0.01\n",
      "episode: 176/300, score: -200, e: 0.01\n",
      "episode: 177/300, score: -200, e: 0.01\n",
      "episode: 178/300, score: -200, e: 0.01\n",
      "episode: 179/300, score: -200, e: 0.01\n",
      "episode: 180/300, score: -200, e: 0.01\n",
      "episode: 181/300, score: -200, e: 0.01\n",
      "episode: 182/300, score: -200, e: 0.01\n",
      "episode: 183/300, score: -200, e: 0.01\n",
      "episode: 184/300, score: -200, e: 0.01\n",
      "episode: 185/300, score: -200, e: 0.01\n",
      "episode: 186/300, score: -200, e: 0.01\n",
      "episode: 187/300, score: -200, e: 0.01\n",
      "episode: 188/300, score: -200, e: 0.01\n",
      "episode: 189/300, score: -200, e: 0.01\n",
      "episode: 190/300, score: -200, e: 0.01\n",
      "episode: 191/300, score: -200, e: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 192/300, score: -200, e: 0.01\n",
      "episode: 193/300, score: -200, e: 0.01\n",
      "episode: 194/300, score: -187, e: 0.01\n",
      "episode: 195/300, score: -200, e: 0.01\n",
      "episode: 196/300, score: -200, e: 0.01\n",
      "episode: 197/300, score: -200, e: 0.01\n",
      "episode: 198/300, score: -200, e: 0.01\n",
      "episode: 199/300, score: -200, e: 0.01\n",
      "episode: 200/300, score: -200, e: 0.01\n",
      "episode: 201/300, score: -200, e: 0.01\n",
      "episode: 202/300, score: -200, e: 0.01\n",
      "episode: 203/300, score: -200, e: 0.01\n",
      "episode: 204/300, score: -200, e: 0.01\n",
      "episode: 205/300, score: -200, e: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 149>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    150\u001b[0m     agent \u001b[38;5;241m=\u001b[39m DQNAgent(env)\n\u001b[1;32m--> 151\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtest()\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mDQNAgent.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done :\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, score: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, e: \u001b[39m\u001b[38;5;132;01m{:.2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_ep, \u001b[38;5;241m-\u001b[39mi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps))\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# If the score reached is better than the previous one\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m>\u001b[39mbesti:\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         target[i][action[i]] \u001b[38;5;241m=\u001b[39m reward[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mamax(target_next[i]))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Train the Neural Network with batches\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1334\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1328\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mcoordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1329\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[0;32m   1332\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1333\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1334\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1350\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1399\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[0;32m   1148\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:328\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    326\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 328\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\data_adapter.py:360\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    358\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data)\n\u001b[1;32m--> 360\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    365\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2018\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2016\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2018\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2019\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2020\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2021\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2022\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2023\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2024\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5251\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[0;32m   5250\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m _validate_and_encode(name)\n\u001b[1;32m-> 5251\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mparallel_map_dataset_v2(\n\u001b[0;32m   5252\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   5254\u001b[0m     f\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m   5255\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls,\n\u001b[0;32m   5256\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic,\n\u001b[0;32m   5257\u001b[0m     use_inter_op_parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m   5258\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   5259\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n\u001b[0;32m   5260\u001b[0m \u001b[38;5;28msuper\u001b[39m(ParallelMapDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:5795\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   5793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   5794\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5795\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5796\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParallelMapDatasetV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5797\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5798\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_inter_op_parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5799\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeterministic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5800\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreserve_cardinality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   5802\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "#Creating the environment of the game\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "#Defining the neural network model\n",
    "def NNModel(states, actions):\n",
    "    #Input layer\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(states)))    \n",
    "    #Hidden layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #Output layer\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    #Configures the model for training\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Our agent\n",
    "class DQNAgent:\n",
    "    def __init__(self,env):\n",
    "        self.env=env\n",
    "        self.states = env.observation_space.shape[0]\n",
    "        self.actions = env.action_space.n\n",
    "        \n",
    "        #Parameters\n",
    "        self.max_ep = 300\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma=0.99\n",
    "        self.eps = 1.0\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        self.model=NNModel(states=(self.states,), actions = self.actions)\n",
    "        \n",
    "    #Adding an entry to our buffer   \n",
    "    def saveToBuff(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > 1000:\n",
    "            if self.eps > 0.01:\n",
    "                #Decreasing eps (the probability to take a random action)\n",
    "                self.eps = self.eps * 0.999\n",
    "                \n",
    "    #Trains the model with experiences from memory\n",
    "    def replay(self):\n",
    "        if len(self.memory) < 1000:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.states))\n",
    "        next_state = np.zeros((self.batch_size, self.states))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Update the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "    \n",
    "    def run(self):\n",
    "        besti=-201\n",
    "        for e in range(self.max_ep):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.states])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #Uncomment to display the task\n",
    "                #self.env.render()\n",
    "                \n",
    "                #Pick a random action if random between 0 and 1 is smaller than eps\n",
    "                if np.random.random() <= self.eps:\n",
    "                    action = random.randrange(self.actions)\n",
    "                else:\n",
    "                    #Pick the best possible action\n",
    "                    action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.states])\n",
    "                #If the Car is still not out or it's the before last step\n",
    "                if done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else : reward=-200\n",
    "                self.saveToBuff(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                #If the Car is reached the objective or run out of steps\n",
    "                if done :\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.max_ep, -i, self.eps))\n",
    "                self.replay() \n",
    "            # If the score reached is better than the previous one\n",
    "            if -i>besti:\n",
    "                besti=-i\n",
    "                #Saving the model\n",
    "                self.save(\"mountain-dqn.h5\")\n",
    "                \n",
    "    def test(self):\n",
    "        #Loading the model\n",
    "        self.model = load_model(\"mountain-dqn.h5\")\n",
    "        SA=0\n",
    "        for e in range(300):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.states])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #Uncomment to display the task\n",
    "                #self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.states])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, 300, i))\n",
    "                    SA+=i\n",
    "                    break\n",
    "        print(\"Score:\", SA/300)\n",
    "        self.env.close()\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgent(env)\n",
    "    agent.run()\n",
    "    agent.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f053f3a",
   "metadata": {},
   "source": [
    "# Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "#Creating the environment of the game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#Parameters to try\n",
    "memory=[2000,10000]\n",
    "gamma=[0.95,0.96,0.97,0.98,0.99,1]\n",
    "batch=[32,64]\n",
    "\n",
    "#Defining the neural network model\n",
    "def NNModel(states, actions):\n",
    "    \n",
    "    #Input layer\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(states)))\n",
    "    \n",
    "    #Hidden layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    \n",
    "    #Output layer\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    \n",
    "    #Configures the model for training\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Our agent\n",
    "class DQNAgent:\n",
    "    def __init__(self,env,memory,gamma,batch):\n",
    "        self.env=env\n",
    "        self.states = env.observation_space.shape[0]\n",
    "        self.actions = env.action_space.n\n",
    "        \n",
    "        #Parameters\n",
    "        self.max_ep = 300\n",
    "        self.memory = deque(maxlen=memory)\n",
    "        self.gamma=gamma\n",
    "        self.eps = 1.0\n",
    "        self.batch_size = batch\n",
    "        \n",
    "        self.m=memory\n",
    "        self.g=gamma\n",
    "        self.b=batch\n",
    "        \n",
    "        self.model=NNModel(states=(self.states,), actions = self.actions)\n",
    "        \n",
    "    #Adding an entry to our buffer\n",
    "    def saveToBuff(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > 1000:\n",
    "            if self.eps > 0.01:\n",
    "                #Decreasing eps (the probability to take a random action)\n",
    "                self.eps = self.eps * 0.99\n",
    "\n",
    "    #Trains the model with experiences from memory\n",
    "    def replay(self):\n",
    "        if len(self.memory) < 1000:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.states))\n",
    "        next_state = np.zeros((self.batch_size, self.states))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # Batch prediction\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Update the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.max_ep):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.states])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                #Uncomment to display the task \n",
    "                #self.env.render()\n",
    "                \n",
    "                #Pick a random action if random between 0 and 1 is smaller than eps\n",
    "                if np.random.random() <= self.eps:\n",
    "                    action = random.randrange(self.actions)\n",
    "                else:\n",
    "                    #Pick the best possible action\n",
    "                    action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.states])\n",
    "                #if the CartPole is still not out or it's the before last step\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.saveToBuff(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                #If the CartPole is out or the score reached 500\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.max_ep, i, self.eps))\n",
    "                    #If the max score is reached\n",
    "                    if i == 500:\n",
    "                        #Saving the model\n",
    "                        self.model.save(\"cartpole-dqn\"+\" \"+str(self.m)+\" \"+str(self.g)+\" \"+str(self.b)+\".h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "    \n",
    "    def test(self):\n",
    "            #Loading the model\n",
    "            self.model = load_model(\"cartpole-dqn\"+\" \"+str(self.m)+\" \"+str(self.g)+\" \"+str(self.b)+\".h5\")\n",
    "            #Score Average\n",
    "            SA=0\n",
    "            for e in range(20):\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.states])\n",
    "                done = False\n",
    "                i = 0\n",
    "                while not done:\n",
    "                    #Uncomment to display the task \n",
    "                    #self.env.render()\n",
    "                    action = np.argmax(self.model.predict(state))\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    state = np.reshape(next_state, [1, self.states])\n",
    "                    i += 1\n",
    "                    if done:\n",
    "                        print(\"episode: {}/{}, score: {}\".format(e, 20, i))\n",
    "                        SA+=i\n",
    "                        break\n",
    "            return SA/20\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    for i in memory:\n",
    "        for j in gamma:\n",
    "            for k in batch:\n",
    "                agent = DQNAgent(env,memory=i,gamma=j,batch=k)\n",
    "                agent.run()\n",
    "    scores_table = pd.DataFrame(columns=[\"memory\",\"gamma\",\"batch\",\"score avg.\"])\n",
    "    for i in memory:\n",
    "        for j in gamma:\n",
    "            for k in batch:\n",
    "                agent = DQNAgent(env,memory=i,gamma=j,batch=k)\n",
    "                agent.run()\n",
    "                agent = DQNAgent(env,memory=i,gamma=j,batch=k)\n",
    "                scores_table=scores_table.append({\"memory\":i,\"gamma\":j,\"batch\":k,\"score avg.\":agent.test()},ignore_index=True)\n",
    "    display(scores_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
